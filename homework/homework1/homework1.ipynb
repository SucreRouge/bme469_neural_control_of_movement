{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Homework 1**\n",
    "\n",
    "by Titipat Achakulvisut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **problem 1**\n",
    "\n",
    "(a) Create a data set for a one dimensional linear mapping corrupted by Gaussian noise (i.e. a mapping between visual and proprioceptive sensory input). In particular, create: $y = 2*x + e$; where $x$ is drawn from a uniform random distribution (using unifrnd) between -10 and 10 with 1000 samples, and e is a normally distributed noise vector with mean zero and standard deviation of 1 (use normrnd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = 20*np.random.rand(1000) - 10 # uniform distribution from -10 to 10\n",
    "e = np.random.normal(0, 1, size=1000)\n",
    "y = 2*x + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, '.', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Create a set of RBFs. Place their centers at -12 to 12 at every .5 along the $x$ axis. Set the standard deviation of each RBF to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rbf_kernel(x, c, sigma=1.):\n",
    "    \"\"\"\n",
    "    Compute radial basis kernel of given domain x\n",
    "    k(x, y) = exp(-(1/sigma**2)*||x-c||^2)\n",
    "    \"\"\"\n",
    "    return np.exp(-(1./(2*sigma**2))*(x - c)**2)\n",
    "\n",
    "def pinv(A):\n",
    "    \"\"\"Psuedo inverse of matrix A\"\"\"\n",
    "    return np.linalg.inv(A.T.dot(A)).dot(A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centers = np.arange(-12, 12.5, 0.5) # center of RBFs\n",
    "x_kernel = np.vstack([rbf_kernel(x, center) for center in centers]).T # map x to set of RBFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Use linear regression to find W for the weighting of each RBF and show the predicted values of $y$ for each $x$ value. Make a plot with the original $x$ vs $y$ data and the $x$ vs predicted $y$ superimposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = np.linalg.pinv(x_kernel).dot(y) # compute W using psuedo inverse\n",
    "y_hat = x_kernel.dot(W) # predicted y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, '.b', alpha=0.2)\n",
    "plt.plot(x, y_hat, '.r', alpha=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['original data', 'predicted data'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Add 50 data points to your $x$ vector, all at $x = 6$. Add a corresponding 50 points to your y vector, according to: $y = 2*x + 10 + e$. (the x used here should correspond to the newly added data points) I.e. displace some of the $y$ values at a particular value of $x$ – as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_concat = 6*np.ones(50)\n",
    "y_concat = 2*x_concat + 10 + np.random.randn(len(x_concat))\n",
    "x_new = np.hstack((x, x_concat))\n",
    "y_new = np.hstack((y, y_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x_new, y_new, '.', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centers = np.arange(-12, 12.5, 0.5) # center of RBFs\n",
    "x_new_kernel = np.vstack([rbf_kernel(x_new, center) for center in centers]).T # map x to set of RBFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = np.linalg.pinv(x_new_kernel).dot(y_new) # compute W using psuedo inverse\n",
    "y_new_hat = x_new_kernel.dot(W) # predicted y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_new, y_new, '.b', alpha=0.2)\n",
    "plt.plot(x_new, y_new_hat, '.r', alpha=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['original data', 'predicted data'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **problem 2**\n",
    "\n",
    "generate 2 classes data (port from given `.m` file to python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Linear classification, 2 classes**\n",
    "\n",
    "Your job is to create a classifier which produces the correct class labels for each data point. You’ve been provided with the ‘correct’ class labels in the variable `y` in the mfile. Use a linear classifier/perceptron: `y_hat = sign(wx)`. Note that the offset should be included in the `x` data vector (i.e. add another column of 1’s to your data, as for regression). Use gradient descent on the LMS error to find the best fit weights for this classification. Plot the identified weight vector on the same plot as the raw data, incorporating the offset when you plot the vector (i.e. it shouldn’t go through zero). Interpret the result of your classifier according to the position of this weight vector. Also plot the separation boundary for this classifier (i.e.the line orthogonal to the weight vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.vstack([np.ones((100, 1)), -1*np.ones((100, 1))]).flatten()\n",
    "X1 = np.hstack((np.random.normal(6, 2, size=(100,1)), np.random.normal(2, 2, size=(100, 1))))\n",
    "X2 = np.hstack((np.random.normal(2, 3, size=(100,1)), np.random.normal(8, 2, size=(100, 1))))\n",
    "X = np.vstack((X1, X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[(y==1).flatten(), 0], X[(y==1).flatten(), 1], color='r')\n",
    "plt.scatter(X[(y==-1).flatten(), 0], X[(y==-1).flatten(), 1], color='b')\n",
    "plt.title('Generated data for problem 2b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = len(X)\n",
    "X = np.hstack((np.ones((m, 1)), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_grad(z):\n",
    "    return sigmoid(z)*(1 - sigmoid(z))\n",
    "\n",
    "def compute_grad(X, y, theta):\n",
    "    \"\"\"Compute gradient\"\"\"\n",
    "    m = len(X)\n",
    "    theta_grad = np.zeros_like(theta)\n",
    "    y_hat = sigmoid(X.dot(theta))\n",
    "    for i in range(0, X.shape[1]):\n",
    "        theta_grad[i] = -2*np.sum((y - y_hat)*sigmoid_grad(y_hat)*X[:, i])\n",
    "    return theta_grad\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    y_hat = sigmoid(X.dot(theta))\n",
    "    return np.sum(y*np.log(y_hat) + (1-y)*np.log(1 - y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "J = [] # history of cost\n",
    "theta = np.array([1, 1, 1]) # intial theta\n",
    "n_iter = 2000 # number of iteration\n",
    "alpha = 0.01 # learning rate\n",
    "for i in range(n_iter):\n",
    "    theta_grad = compute_grad(X, y, theta)\n",
    "    theta = theta - alpha*theta_grad\n",
    "    #J.append(compute_cost(X, y, theta))\n",
    "print('final weight theta = ', theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Linear classification, 2 classes, sigmoidal output** \n",
    "\n",
    "Here your classifier will act according to: `y_hat = sigmoid(wx)`. Make the same plots as in (a). In addition, plot the output of the sigmoid (i.e. `y_hat`) and interpret its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_grad(z):\n",
    "    return sigmoid(z)*(1 - sigmoid(z))\n",
    "\n",
    "def compute_grad(X, y, theta):\n",
    "    \"\"\"Compute gradient\"\"\"\n",
    "    m = len(X)\n",
    "    theta_grad = np.zeros_like(theta)\n",
    "    y_hat = sigmoid(X.dot(theta))\n",
    "    for i in range(0, X.shape[1]):\n",
    "        theta_grad[i] = -2*np.sum((y - y_hat)*sigmoid_grad(y_hat)*X[:, i])\n",
    "    return theta_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.vstack([np.ones((100, 1)), np.zeros((100, 1))]).flatten()\n",
    "X1 = np.hstack((np.random.normal(6, 2, size=(100,1)), np.random.normal(2, 2, size=(100, 1))))\n",
    "X2 = np.hstack((np.random.normal(2, 3, size=(100,1)), np.random.normal(8, 2, size=(100, 1))))\n",
    "X = np.vstack((X1, X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[(y==1).flatten(), 0], X[(y==1).flatten(), 1], color='r')\n",
    "plt.scatter(X[(y==0).flatten(), 0], X[(y==0).flatten(), 1], color='b')\n",
    "plt.title('Generated data for problem 2b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = len(X)\n",
    "X = np.hstack((np.ones((m, 1)), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "theta = np.array([1, 1, 1]) # intial theta (weight)\n",
    "n_iter = 5000 # number of iteration\n",
    "alpha = 0.05 # learning rate\n",
    "for i in range(n_iter):\n",
    "    theta_grad = compute_grad(X, y, theta)\n",
    "    theta = theta - alpha*theta_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = np.round(sigmoid(X.dot(theta))) # predicted class\n",
    "print('Total correct classified = %s or %s percent' % (np.sum(y == y_hat), 100*np.sum(y == y_hat)/m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Linear classification, 3 classes, sigmoidal outputs**\n",
    "\n",
    "Use the code in ‘ps1 3 classes sigmoid lda’ (3rd cell) to generate the data. Here there are three classes of data. There are three output units for the network–one for each class. When a data point is generated from the first class, the output is `[1 0 0];` from the second class, the output is `[0 1 0];` from the third class, the output is `[0 0 1]`. This is all in the variable y in the code. Your network should act according to `y_hat = sigmoid(Wx)`, where `y` is now a vector and `W` is a 3 by 3 matrix (3 dimensional output and 3 dimensional input). The predicted class for each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classfier - back propagation** \n",
    "\n",
    "Your job is to create code implementing back propagation for a two layer neural network which can perform this classification. Use a network with 4 hidden units, as indicated in the shell code. Don’t worry about cross validation and all that – feel free to just use all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Note for Matlab code to generate datasets for problem**\n",
    "\n",
    "```\n",
    "%% data for problem #2a\n",
    "\n",
    "clear\n",
    "dat1 = [normrnd(6,2,100,1) normrnd(2,1,100,1)];   % create the input data\n",
    "dat2 = [normrnd(2,3,100,1) normrnd(8,1,100,1)];\n",
    "dat = [dat1; dat2];\n",
    "\n",
    "y = [ones(100,1); -1*ones(100,1)];   % these are the labels for the classes\n",
    "\n",
    "% plot the data\n",
    "ind1 = find(y == 1);\n",
    "ind2 = find(y == -1);\n",
    "plot(dat(ind1,1),dat(ind1,2),'r.')\n",
    "hold on\n",
    "plot(dat(ind2,1),dat(ind2,2),'b.')\n",
    "hold off\n",
    "\n",
    "%% data for problem #2b\n",
    "\n",
    "clear\n",
    "dat1 = [normrnd(6,2,100,1) normrnd(2,2,100,1)];  % create the input data\n",
    "dat2 = [normrnd(2,3,100,1) normrnd(8,2,100,1)];\n",
    "dat = [dat1; dat2];\n",
    "\n",
    "y = [ones(100,1); 0*ones(100,1)];  % labels for classes\n",
    "\n",
    "% plot the data\n",
    "ind1 = find(y == 1);\n",
    "ind2 = find(y == 0);\n",
    "plot(dat(ind1,1),dat(ind1,2),'r.')\n",
    "hold on\n",
    "plot(dat(ind2,1),dat(ind2,2),'b.')\n",
    "hold off\n",
    "\n",
    "%% data for problem #2c\n",
    "\n",
    "clear\n",
    "dat1 = [normrnd(6,1,100,1) normrnd(2,1,100,1)];      % create the input data\n",
    "dat2 = [normrnd(2,1,100,1) normrnd(8,1,100,1)];\n",
    "dat3 = [normrnd(-2,1,100,1) normrnd(-2,1,100,1)];\n",
    "dat = [dat1; dat2; dat3];\n",
    "\n",
    "y(:,1) = [ones(100,1); zeros(100,1); zeros(100,1)];   % the class labels as three dimensional outputs\n",
    "y(:,2) = [zeros(100,1); ones(100,1); zeros(100,1)]';\n",
    "y(:,3) = [zeros(100,1); zeros(100,1); ones(100,1)]';\n",
    "    \n",
    "% plot the data\n",
    "ind1 = find(y(:,1) == 1);\n",
    "ind2 = find(y(:,2) == 1);\n",
    "ind3 = find(y(:,3) == 1);\n",
    "plot(dat(ind1,1),dat(ind1,2),'r.')\n",
    "hold on\n",
    "plot(dat(ind2,1),dat(ind2,2),'b.')\n",
    "plot(dat(ind3,1),dat(ind3,2),'g.')\n",
    "hold off\n",
    "\n",
    "%% data for problem 3\n",
    "\n",
    "clear\n",
    "sd = .85;\n",
    "\n",
    "x1 = [normrnd(0,sd,50,1); normrnd(0,sd,50,1);  normrnd(0,sd,50,1)];   % the input data\n",
    "x2 = [normrnd(0,sd,50,1); normrnd(5,sd,50,1);   normrnd(10,sd,50,1)];\n",
    "x3 = [ones(150,1)];\n",
    "dat = [x1 x2 x3];\n",
    "y = [ones(50,1) zeros(50,1) zeros(50,1);  zeros(50,1) ones(50,1) zeros(50,1); zeros(50,1) zeros(50,1) ones(50,1) ];  % the labels\n",
    "\n",
    "% plot the data\n",
    "ind1 = find(y(:,1) == 1);\n",
    "ind2 = find(y(:,2) == 1);\n",
    "ind3 = find(y(:,3) == 1);\n",
    "plot(dat(ind1,1),dat(ind1,2),'r.')\n",
    "hold on\n",
    "plot(dat(ind2,1),dat(ind2,2),'b.')\n",
    "plot(dat(ind3,1),dat(ind3,2),'g.')\n",
    "hold off\n",
    "\n",
    "% set up the starting parameters for the 2 layer network and weights\n",
    "nsamp = length(x1);\n",
    "ninput = 3;\n",
    "nhidden = 4;\n",
    "noutput = 3;\n",
    "\n",
    "W = unifrnd(-1,1,ninput,nhidden);  % initialize weight matrices\n",
    "V = unifrnd(-1,1,nhidden,noutput);\n",
    "\n",
    "mu = .05; p = .9;   % a suggested step and momentum size\n",
    "\n",
    "lastdW = 0*W;  lastdV = 0*V;   % initialize the previous weight change variables, to be used for the momentum\n",
    "\n",
    "% now do back prop\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
